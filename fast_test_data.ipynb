{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi18TfXZhZWr",
        "outputId": "ba2eeabc-8494-439d-8b79-e83a793fecb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "from typing import Callable, Sequence, Tuple, Union\n",
        "import json\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.ops import sigmoid_focal_loss, batched_nms\n",
        "from torchvision.ops.misc import FrozenBatchNorm2d\n",
        "\n",
        "# Googleドライブをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/python_image_recognition/5_object_detection/5_3_retinanet')\n",
        "\n",
        "import util\n",
        "import dataset\n",
        "import transform as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    '''\n",
        "    ResNet18における残差ブロック\n",
        "    in_channels : 入力チャネル数\n",
        "    out_channels: 出力チャネル数\n",
        "    stride      : 畳み込み層のストライド\n",
        "    '''\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 stride: int=1):\n",
        "        super().__init__()\n",
        "\n",
        "        ''''' 残差接続 '''''\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = FrozenBatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = FrozenBatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        ''''''''''''''''''''\n",
        "\n",
        "        # strideが1より大きいときにスキップ接続と残差接続の高さと幅を\n",
        "        # 合わせるため、別途畳み込み演算を用意\n",
        "        self.downsample = None\n",
        "        if stride > 1:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                FrozenBatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    '''\n",
        "    順伝播関数\n",
        "    x: 入力, [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "    '''\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        ''''' 残差接続 '''''\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        ''''''''''''''''''''\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        # 残差写像と恒等写像の要素毎の和を計算\n",
        "        out += x\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "mGQ3OgZVjQUW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "    '''\n",
        "    ResNet18モデル\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2,\n",
        "                               padding=3, bias=False)\n",
        "        self.bn1 = FrozenBatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3,\n",
        "                                     stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            BasicBlock(64, 64),\n",
        "            BasicBlock(64, 64),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            BasicBlock(64, 128, stride=2),\n",
        "            BasicBlock(128, 128),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            BasicBlock(128, 256, stride=2),\n",
        "            BasicBlock(256, 256),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            BasicBlock(256, 512, stride=2),\n",
        "            BasicBlock(512, 512),\n",
        "        )\n",
        "\n",
        "    '''\n",
        "    順伝播関数\n",
        "    x: 入力, [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "    '''\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        c3 = self.layer2(x)\n",
        "        c4 = self.layer3(c3)\n",
        "        c5 = self.layer4(c4)\n",
        "\n",
        "        return c3, c4, c5"
      ],
      "metadata": {
        "id": "FXeZwdUKjWPK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeaturePyramidNetwork(nn.Module):\n",
        "    '''\n",
        "    特徴ピラミッドネットワーク\n",
        "    num_features: 出力特徴量のチャネル数\n",
        "    '''\n",
        "    def __init__(self, num_features: int=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # 特徴ピラミッドネットワークから出力される階層レベル\n",
        "        # バックボーンネットワークの最終層の特徴マップを第5階層とし、\n",
        "        # 縮小方向に第6, 7階層の2つの特徴マップを、\n",
        "        # 拡大方向に第3, 4階層の2つの特徴マップを生成\n",
        "        self.levels = (3, 4, 5, 6, 7)\n",
        "\n",
        "        ''' 縮小方向の特徴抽出 '''\n",
        "\n",
        "        self.p6 = nn.Conv2d(512, num_features, kernel_size=3,\n",
        "                            stride=2, padding=1)\n",
        "\n",
        "        self.p7_relu = nn.ReLU(inplace=True)\n",
        "        self.p7 = nn.Conv2d(num_features, num_features, kernel_size=3,\n",
        "                            stride=2, padding=1)\n",
        "\n",
        "        ''''''''''''''''''''''''''\n",
        "\n",
        "        ''' 拡大方向の特徴抽出 '''\n",
        "\n",
        "        self.p5_1 = nn.Conv2d(512, num_features, kernel_size=1)\n",
        "        self.p5_2 = nn.Conv2d(num_features, num_features,\n",
        "                              kernel_size=3, padding=1)\n",
        "\n",
        "        self.p4_1 = nn.Conv2d(256, num_features, kernel_size=1)\n",
        "        self.p4_2 = nn.Conv2d(num_features, num_features,\n",
        "                              kernel_size=3, padding=1)\n",
        "\n",
        "        self.p3_1 = nn.Conv2d(128, num_features, kernel_size=1)\n",
        "        self.p3_2 = nn.Conv2d(num_features, num_features,\n",
        "                              kernel_size=3, padding=1)\n",
        "\n",
        "        ''''''''''''''''''''''''''\n",
        "\n",
        "    '''\n",
        "    順伝播関数\n",
        "    c3: 特徴マップ, [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "    c4: 特徴マップ, [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "    c5: 特徴マップ, [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "    '''\n",
        "    def forward(self, c3: torch.Tensor, c4: torch.Tensor,\n",
        "                c5: torch.Tensor):\n",
        "        ''' 縮小方向の特徴抽出 '''\n",
        "\n",
        "        p6 = self.p6(c5)\n",
        "\n",
        "        p7 = self.p7_relu(p6)\n",
        "        p7 = self.p7(p7)\n",
        "\n",
        "        ''''''''''''''''''''''''''\n",
        "\n",
        "        ''' 拡大方向の特徴抽出 '''\n",
        "\n",
        "        p5 = self.p5_1(c5)\n",
        "        p5_up = F.interpolate(p5, scale_factor=2)\n",
        "        p5 = self.p5_2(p5)\n",
        "\n",
        "        p4 = self.p4_1(c4) + p5_up\n",
        "        p4_up = F.interpolate(p4, scale_factor=2)\n",
        "        p4 = self.p4_2(p4)\n",
        "\n",
        "        p3 = self.p3_1(c3) + p4_up\n",
        "        p3 = self.p3_2(p3)\n",
        "\n",
        "        ''''''''''''''''''''''''''\n",
        "\n",
        "        return p3, p4, p5, p6, p7"
      ],
      "metadata": {
        "id": "gl5APNcBja7f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionHead(nn.Module):\n",
        "    '''\n",
        "    検出ヘッド(分類や矩形の回帰に使用)\n",
        "    num_channels_per_anchor: 1アンカーに必要な出力チャネル数\n",
        "    num_anchors            : アンカー数\n",
        "    num_features           : 入力および中間特徴量のチャネル数\n",
        "    '''\n",
        "    def __init__(self, num_channels_per_anchor: int,\n",
        "                 num_anchors: int=9, num_features: int=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_anchors = num_anchors\n",
        "\n",
        "        # 特徴ピラミッドネットワークの特徴マップを分類や回帰専用の\n",
        "        # 特徴マップに変換するための畳み込みブロック\n",
        "        self.conv_blocks = nn.ModuleList([\n",
        "            nn.Sequential(nn.Conv2d(num_features, num_features,\n",
        "                                    kernel_size=3, padding=1),\n",
        "                          nn.ReLU(inplace=True))\n",
        "            for _ in range(4)])\n",
        "\n",
        "        # 予測結果をアンカーボックスの数だけ用意、例えば分類ヘッドの\n",
        "        # 場合、チャネルをアンカーボックス数*物体クラス数に設定\n",
        "        self.out_conv = nn.Conv2d(\n",
        "            num_features, num_anchors * num_channels_per_anchor,\n",
        "            kernel_size=3, padding=1)\n",
        "\n",
        "    '''\n",
        "    順伝播関数\n",
        "    x: 特徴マップ, [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "    '''\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        for i in range(4):\n",
        "            x = self.conv_blocks[i](x)\n",
        "        x = self.out_conv(x)\n",
        "\n",
        "        bs, c, h, w = x.shape\n",
        "\n",
        "        # 後処理に備えて予測結果を並び替え\n",
        "        # permute関数により、\n",
        "        # [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "        # -> [バッチサイズ, 高さ, 幅, チャネル数]\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        # 第1軸に全画素の予測結果を並べる\n",
        "        x = x.reshape(bs, w * h * self.num_anchors, -1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Cb96nTd-jeul"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnchorGenerator:\n",
        "    '''\n",
        "    検出の基準となるアンカーボックスを生成するクラス\n",
        "    levels: 入力特徴マップの階層\n",
        "    '''\n",
        "    def __init__(self, levels: int):\n",
        "        # 用意するアンカーボックスのアスペクト比(ハイパーパラメータ)\n",
        "        ratios = torch.tensor([0.5, 1.0, 2.0])\n",
        "\n",
        "        # 用意するアンカーボックスの基準となる大きさに\n",
        "        # 対するスケール(ハイパーパラメータ)\n",
        "        scales = torch.tensor([2 ** 0, 2 ** (1 / 3), 2 ** (2 / 3)])\n",
        "\n",
        "        # 1つのアスペクト比に対して全スケールのアンカーボックスを\n",
        "        # 用意するので、アンカーボックスの数は\n",
        "        # アスペクト比の数 * スケール数になる\n",
        "        self.num_anchors = ratios.shape[0] * scales.shape[0]\n",
        "\n",
        "        # 各階層の特徴マップでの1画素での移動量が入力画像での\n",
        "        # 何画素の移動になるかを表す数値\n",
        "        self.strides = [2 ** level for level in levels]\n",
        "\n",
        "        self.anchors = []\n",
        "        for level in levels:\n",
        "            # 現階層における基準となる正方形のアンカーボックスの一辺の長さ\n",
        "            # 深い階層のアンカーボックスには大きい物体の\n",
        "            # 検出を担当させるため、基準の長さを長く設定\n",
        "            base_length = 2 ** (level + 2)\n",
        "\n",
        "            # アンカーボックスの1辺の長さをスケール\n",
        "            scaled_lengths = base_length * scales\n",
        "            # アンカーボックスが正方形の場合の面積を計算\n",
        "            anchor_areas = scaled_lengths ** 2\n",
        "\n",
        "            # アスペクト比(ratio)に応じて辺の長さを変更\n",
        "            # width * height = width * (width * ratio) = areaより\n",
        "            # width = (area / ratio) ** 0.5\n",
        "            # unsqueezeとブロードキャストにより\n",
        "            # アスペクト比の数 * スケール数の数のアンカーボックスの\n",
        "            # 幅と高さを生成\n",
        "            anchor_widths = (\n",
        "                anchor_areas / ratios.unsqueeze(1)) ** 0.5\n",
        "            anchor_heights = anchor_widths * ratios.unsqueeze(1)\n",
        "\n",
        "            # アスペクト比の数 * スケール数の行列を平坦化\n",
        "            anchor_widths = anchor_widths.flatten()\n",
        "            anchor_heights = anchor_heights.flatten()\n",
        "\n",
        "            # アンカーボックスの中心を原点としたときの\n",
        "            # xmin, ymin, xmax, ymaxのオフセットをを計算\n",
        "            anchor_xmin = -0.5 * anchor_widths\n",
        "            anchor_ymin = -0.5 * anchor_heights\n",
        "            anchor_xmax = 0.5 * anchor_widths\n",
        "            anchor_ymax = 0.5 * anchor_heights\n",
        "\n",
        "            level_anchors = torch.stack(\n",
        "                (anchor_xmin, anchor_ymin, anchor_xmax, anchor_ymax),\n",
        "                dim=1)\n",
        "\n",
        "            self.anchors.append(level_anchors)\n",
        "\n",
        "    '''\n",
        "    アンカーボックス生成関数\n",
        "    feature_sizes: 入力される複数の特徴マップそれぞれの大きさ\n",
        "    '''\n",
        "    # 関数内で勾配を計算する必要がないことを明示\n",
        "    @torch.no_grad()\n",
        "    def generate(self, feature_sizes: Sequence[torch.Size]):\n",
        "        # 各階層の特徴マップの大きさを入力として、\n",
        "        # 特徴マップの各画素におけるアンカーボックスを生成\n",
        "        anchors = []\n",
        "        for stride, level_anchors, feature_size in zip(\n",
        "                self.strides, self.anchors, feature_sizes):\n",
        "            # 現階層の特徴マップの大きさ\n",
        "            height, width = feature_size\n",
        "\n",
        "            # 入力画像の画素の移動量を表すstrideを使って\n",
        "            # 特徴マップの画素の位置 -> 入力画像の画素の位置に変換\n",
        "            # (画素の中心位置を計算するために0.5を加算)\n",
        "            xs = (torch.arange(width) + 0.5) * stride\n",
        "            ys = (torch.arange(height) + 0.5) * stride\n",
        "\n",
        "            # x座標のリストととy座標のリストをを組み合わせて\n",
        "            # グリッド上の座標を生成\n",
        "            grid_x, grid_y = torch.meshgrid(xs, ys, indexing='xy')\n",
        "\n",
        "            grid_x = grid_x.flatten()\n",
        "            grid_y = grid_y.flatten()\n",
        "\n",
        "            # 各画素の中心位置にアンカーボックスの\n",
        "            # xmin, ymin, xmax, ymaxのオフセットを加算\n",
        "            anchor_xmin = (\n",
        "                grid_x.unsqueeze(1) + level_anchors[:, 0]).flatten()\n",
        "            anchor_ymin = (\n",
        "                grid_y.unsqueeze(1) + level_anchors[:, 1]).flatten()\n",
        "            anchor_xmax = (\n",
        "                grid_x.unsqueeze(1) + level_anchors[:, 2]).flatten()\n",
        "            anchor_ymax = (\n",
        "                grid_y.unsqueeze(1) + level_anchors[:, 3]).flatten()\n",
        "\n",
        "            # 第1軸を追加してxmin, ymin, xmax, ymaxを連結\n",
        "            level_anchors = torch.stack(\n",
        "                (anchor_xmin, anchor_ymin, anchor_xmax, anchor_ymax),\n",
        "                dim=1)\n",
        "\n",
        "            anchors.append(level_anchors)\n",
        "\n",
        "        # 全階層のアンカーボックスを連結\n",
        "        anchors = torch.cat(anchors)\n",
        "\n",
        "        return anchors"
      ],
      "metadata": {
        "id": "pUuFRM9NjlH1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RetinaNet(nn.Module):\n",
        "    '''\n",
        "    RetinaNetモデル(ResNet18バックボーン)\n",
        "    num_classes: 物体クラス数\n",
        "    '''\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = ResNet18()\n",
        "\n",
        "        self.fpn = FeaturePyramidNetwork()\n",
        "\n",
        "        self.anchor_generator = AnchorGenerator(self.fpn.levels)\n",
        "\n",
        "        # 分類および矩形ヘッド\n",
        "        # 検出ヘッドは全ての特徴マップで共有\n",
        "        self.class_head = DetectionHead(\n",
        "            num_channels_per_anchor=num_classes,\n",
        "            num_anchors=self.anchor_generator.num_anchors)\n",
        "\n",
        "        # num_channels_per_anchor=4は\n",
        "        # (x_diff, y_diff, w_diff, h_diff)を推論するため\n",
        "        self.box_head = DetectionHead(\n",
        "            num_channels_per_anchor=4,\n",
        "            num_anchors=self.anchor_generator.num_anchors)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    '''\n",
        "    パラメータの初期化関数\n",
        "    '''\n",
        "    def _reset_parameters(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out',\n",
        "                                        nonlinearity='relu')\n",
        "\n",
        "        # 分類ヘッドの出力にシグモイドを適用して各クラスの確率を出力\n",
        "        # 学習開始時の確率が0.01になるようにパラメータを初期化\n",
        "        prior = torch.tensor(0.01)\n",
        "        nn.init.zeros_(self.class_head.out_conv.weight)\n",
        "        nn.init.constant_(self.class_head.out_conv.bias,\n",
        "                          -((1.0 - prior) / prior).log())\n",
        "\n",
        "        # 学習開始時のアンカーボックスの中心位置の移動が0、\n",
        "        # 大きさが1倍となるように矩形ヘッドを初期化\n",
        "        nn.init.zeros_(self.box_head.out_conv.weight)\n",
        "        nn.init.zeros_(self.box_head.out_conv.bias)\n",
        "\n",
        "    '''\n",
        "    順伝播関数\n",
        "    x: 入力画像, [バッチサイズ, チャネル数, 高さ, 幅]\n",
        "    '''\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        cs = self.backbone(x)\n",
        "        ps = self.fpn(*cs)\n",
        "\n",
        "        preds_class = torch.cat(list(map(self.class_head, ps)), dim=1)\n",
        "        preds_box = torch.cat(list(map(self.box_head, ps)), dim=1)\n",
        "\n",
        "        # 各特徴マップの高さと幅をリストに保持\n",
        "        feature_sizes = [p.shape[2:] for p in ps]\n",
        "        anchors = self.anchor_generator.generate(feature_sizes)\n",
        "        anchors = anchors.to(x.device)\n",
        "\n",
        "        return preds_class, preds_box, anchors\n",
        "\n",
        "    '''\n",
        "    モデルパラメータが保持されているデバイスを返す関数\n",
        "    '''\n",
        "    def get_device(self):\n",
        "        return self.backbone.conv1.weight.device"
      ],
      "metadata": {
        "id": "Q-lg0KljjsRo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "preds_class   : 検出矩形のクラス,\n",
        "                [バッチサイズ, アンカーボックス数, 物体クラス数]\n",
        "preds_box     : 検出矩形のアンカーボックスからの誤差,\n",
        "                [バッチサイズ, アンカーボックス数,\n",
        "                                4 (x_diff, y_diff, w_diff, h_diff)]\n",
        "anchors       : アンカーボックス,\n",
        "                [アンカーボックス数, 4 (xmin, ymin, xmax, ymax)]\n",
        "targets       : ラベル\n",
        "conf_threshold: 信頼度の閾値\n",
        "nms_threshold : NMSのIoU閾値\n",
        "'''\n",
        "@torch.no_grad()\n",
        "def post_process(preds_class: torch.Tensor, preds_box: torch.Tensor,\n",
        "                 anchors: torch.Tensor, targets: dict,\n",
        "                 conf_threshold: float=0.05,\n",
        "                 nms_threshold: float=0.5):\n",
        "    batch_size = preds_class.shape[0]\n",
        "\n",
        "    anchors_xywh = util.convert_to_xywh(anchors)\n",
        "\n",
        "    # 中心座標の予測をスケール不変にするため、\n",
        "    # 予測値をアンカーボックスの大きさでスケールする\n",
        "    preds_box[:, :, :2] = anchors_xywh[:, :2] + \\\n",
        "        preds_box[:, :, :2] * anchors_xywh[:, 2:]\n",
        "    preds_box[:, :, 2:] = preds_box[:, :, 2:].exp() * \\\n",
        "        anchors_xywh[:, 2:]\n",
        "\n",
        "    preds_box = util.convert_to_xyxy(preds_box)\n",
        "\n",
        "    # 物体クラスの予測確率をシグモイド関数で計算\n",
        "    # RetinaNetでは背景クラスは存在せず、\n",
        "    # 背景を表す場合は全ての物体クラスの予測確率が低くなるように\n",
        "    # 実装されている\n",
        "    preds_class = preds_class.sigmoid()\n",
        "\n",
        "    # forループで画像毎に処理を実施\n",
        "    scores = []\n",
        "    labels = []\n",
        "    boxes = []\n",
        "    for img_preds_class, img_preds_box, img_targets in zip(\n",
        "            preds_class, preds_box, targets):\n",
        "        # 検出矩形が画像内に収まるように座標をクリップ\n",
        "        img_preds_box[:, ::2] = img_preds_box[:, ::2].clamp(\n",
        "            min=0, max=img_targets['size'][0])\n",
        "        img_preds_box[:, 1::2] = img_preds_box[:, 1::2].clamp(\n",
        "            min=0, max=img_targets['size'][1])\n",
        "\n",
        "        # 検出矩形は入力画像の大きさに合わせたものになっているので、\n",
        "        # 元々の画像に合わせて検出矩形をスケール\n",
        "        img_preds_box *= img_targets['orig_size'][0] / \\\n",
        "            img_targets['size'][0]\n",
        "\n",
        "        # 物体クラスのスコアとクラスIDを取得\n",
        "        img_preds_score, img_preds_label = img_preds_class.max(dim=1)\n",
        "\n",
        "        # 信頼度が閾値より高い検出矩形のみを残す\n",
        "        keep = img_preds_score > conf_threshold\n",
        "        img_preds_score = img_preds_score[keep]\n",
        "        img_preds_label = img_preds_label[keep]\n",
        "        img_preds_box = img_preds_box[keep]\n",
        "\n",
        "        # クラス毎にNMSを適用\n",
        "        keep_indices = batched_nms(img_preds_box, img_preds_score,\n",
        "                                   img_preds_label, nms_threshold)\n",
        "\n",
        "        scores.append(img_preds_score[keep_indices])\n",
        "        labels.append(img_preds_label[keep_indices])\n",
        "        boxes.append(img_preds_box[keep_indices])\n",
        "\n",
        "    return scores, labels, boxes"
      ],
      "metadata": {
        "id": "01AphTNOjxt9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "preds_class        : 検出矩形のクラス,\n",
        "                     [バッチサイズ, アンカーボックス数, 物体クラス数]\n",
        "preds_box          : 検出矩形のアンカーボックスからの誤差,\n",
        "                     [バッチサイズ, アンカーボックス数,\n",
        "                                4 (x_diff, y_diff, w_diff, h_diff)]\n",
        "anchors            : アンカーボックス,\n",
        "                     [アンカーボックス数, 4 (xmin, ymin, xmax, ymax)]\n",
        "targets            : ラベル\n",
        "iou_lower_threshold: 検出矩形と正解矩形をマッチさせるか決める下の閾値\n",
        "iou_upper_threshold: 検出矩形と正解矩形をマッチさせるか決める上の閾値\n",
        "'''\n",
        "def loss_func(preds_class: torch.Tensor, preds_box: torch.Tensor,\n",
        "              anchors: torch.Tensor, targets: dict,\n",
        "              iou_lower_threshold: float=0.4,\n",
        "              iou_upper_threshold: float=0.5):\n",
        "    anchors_xywh = util.convert_to_xywh(anchors)\n",
        "\n",
        "    # 画像毎に目的関数を計算\n",
        "    loss_class = preds_class.new_tensor(0)\n",
        "    loss_box = preds_class.new_tensor(0)\n",
        "    for img_preds_class, img_preds_box, img_targets in zip(\n",
        "            preds_class, preds_box, targets):\n",
        "        # 現在の画像に対する正解矩形がないとき\n",
        "        if img_targets['classes'].shape[0] == 0:\n",
        "            # 全ての物体クラスの確率が0となるように\n",
        "            # (背景として分類されるように)ラベルを作成\n",
        "            targets_class = torch.zeros_like(img_preds_class)\n",
        "            loss_class += sigmoid_focal_loss(\n",
        "                img_preds_class, targets_class, reduction='sum')\n",
        "\n",
        "            continue\n",
        "\n",
        "        # 各画素のアンカーボックスと正解矩形のIoUを計算し、\n",
        "        # 各アンカーボックスに対して最大のIoUを持つ正解矩形を抽出\n",
        "        ious = util.calc_iou(anchors, img_targets['boxes'])[0]\n",
        "        ious_max, ious_argmax = ious.max(dim=1)\n",
        "\n",
        "        # 分類のラベルを-1で初期化\n",
        "        # IoUが下の閾値と上の閾値の間にあるアンカーボックスは\n",
        "        # ラベルを-1として損失を計算しないようにする\n",
        "        targets_class = torch.full_like(img_preds_class, -1)\n",
        "\n",
        "        # アンカーボックスとマッチした正解矩形のIoUが下の閾値より\n",
        "        # 小さい場合、全ての物体クラスの確率が0となるようラベルを用意\n",
        "        targets_class[ious_max < iou_lower_threshold] = 0\n",
        "\n",
        "        # アンカーボックスとマッチした正解矩形のIoUが上の閾値より\n",
        "        # 大きい場合、陽性のアンカーボックスとして分類回帰の対象にする\n",
        "        positive_masks = ious_max > iou_upper_threshold\n",
        "        num_positive_anchors = positive_masks.sum()\n",
        "\n",
        "        # 陽性のアンカーボックスについて、マッチした正解矩形が示す\n",
        "        # 物体クラスの確率を1、それ以外を0として出力するように\n",
        "        # ラベルに値を代入\n",
        "        targets_class[positive_masks] = 0\n",
        "        assigned_classes = img_targets['classes'][ious_argmax]\n",
        "        targets_class[positive_masks,\n",
        "                      assigned_classes[positive_masks]] = 1\n",
        "\n",
        "        # IoUが下の閾値と上の閾値の間にあるアンカーボックスについては\n",
        "        # 分類の損失を計算しない\n",
        "        loss_class += ((targets_class != -1) * sigmoid_focal_loss(\n",
        "            img_preds_class, targets_class)).sum() / \\\n",
        "            num_positive_anchors.clamp(min=1)\n",
        "\n",
        "        # 陽性のアンカーボックスが一つも存在しないとき\n",
        "        # 矩形の誤差の学習はしない\n",
        "        if num_positive_anchors == 0:\n",
        "            continue\n",
        "\n",
        "        # 各アンカーボックスにマッチした正解矩形を抽出\n",
        "        assigned_boxes = img_targets['boxes'][ious_argmax]\n",
        "        assigned_boxes_xywh = util.convert_to_xywh(assigned_boxes)\n",
        "\n",
        "        # アンカーボックスとマッチした正解矩形との誤差を計算し、\n",
        "        # ラベルを作成\n",
        "        targets_box = torch.zeros_like(img_preds_box)\n",
        "        # 中心位置の誤差はアンカーボックスの大きさでスケール\n",
        "        targets_box[:, :2] = (\n",
        "            assigned_boxes_xywh[:, :2] - anchors_xywh[:, :2]) / \\\n",
        "            anchors_xywh[:, 2:]\n",
        "        # 大きさはアンカーボックスに対するスケールのlogを予測\n",
        "        targets_box[:, 2:] = (assigned_boxes_xywh[:, 2:] / \\\n",
        "                              anchors_xywh[:, 2:]).log()\n",
        "\n",
        "        # L1誤差とL2誤差を組み合わせたsmooth L1誤差を使用\n",
        "        loss_box += F.smooth_l1_loss(img_preds_box[positive_masks],\n",
        "                                     targets_box[positive_masks],\n",
        "                                     beta=1 / 9)\n",
        "\n",
        "    batch_size = preds_class.shape[0]\n",
        "    loss_class = loss_class / batch_size\n",
        "    loss_box = loss_box / batch_size\n",
        "\n",
        "    return loss_class, loss_box"
      ],
      "metadata": {
        "id": "L1R9v7-Vj1TB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigTrainEval:\n",
        "    '''\n",
        "    ハイパーパラメータとオプションの設定\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.img_directory = 'gazou'                     # 画像があるディレクトリ\n",
        "        self.anno_file = 'drive/MyDrive/python_image_recognition/data/coco2014/' \\\n",
        "                            '1annotations.json' # アノテーションファイルのパス\n",
        "        self.save_file = 'drive/MyDrive/python_image_recognition/5_object_detection/model/' \\\n",
        "                            'retinanet.pth'                # パラメータを保存するパス\n",
        "        self.val_ratio = 0.2                               # 検証に使う学習セット内のデータの割合\n",
        "        self.num_epochs = 50                               # 学習エポック数\n",
        "        self.lr_drop = 45                                  # 学習率を減衰させるエポック\n",
        "        self.val_interval = 5                              # 検証を行うエポック間隔\n",
        "        self.lr = 1e-5                                     # 学習率\n",
        "        self.clip = 0.1                                    # 勾配のクリップ上限\n",
        "        self.moving_avg = 100                              # 移動平均で計算する損失と正確度の値の数\n",
        "        self.batch_size = 8                                # バッチサイズ\n",
        "        self.num_workers = 2                               # データローダに使うCPUプロセスの数\n",
        "        self.device = 'cuda'                               # 学習に使うデバイス\n"
      ],
      "metadata": {
        "id": "hiC2z4Raj4Hp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_eval():\n",
        "    config = ConfigTrainEval()\n",
        "\n",
        "    # データ拡張・整形クラスの設定\n",
        "    min_sizes = (480, 512, 544, 576, 608)\n",
        "    train_transforms = T.Compose((\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomSelect(\n",
        "            T.RandomResize(min_sizes, max_size=1024),\n",
        "            T.Compose((\n",
        "                T.RandomSizeCrop(scale=(0.8, 1.0),\n",
        "                                 ratio=(0.75, 1.333)),\n",
        "                T.RandomResize(min_sizes, max_size=1024),\n",
        "            ))\n",
        "        ),\n",
        "        T.ToTensor(),\n",
        "        # ImageNetデータセットの平均と標準偏差\n",
        "        T.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)),\n",
        "    ))\n",
        "    test_transforms = T.Compose((\n",
        "        # テストは短辺最大で実行\n",
        "        T.RandomResize((min_sizes[-1],), max_size=1024),\n",
        "        T.ToTensor(),\n",
        "        # ImageNetデータセットの平均と標準偏差\n",
        "        T.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)),\n",
        "    ))\n",
        "\n",
        "    # データセットの用意\n",
        "    train_dataset = dataset.CocoDetection(\n",
        "        img_directory=config.img_directory,\n",
        "        anno_file=config.anno_file, transform=train_transforms)\n",
        "    val_dataset = dataset.CocoDetection(\n",
        "        img_directory=config.img_directory,\n",
        "        anno_file=config.anno_file, transform=test_transforms)\n",
        "\n",
        "    # Subset samplerの生成\n",
        "    val_set, train_set = util.generate_subset(\n",
        "        train_dataset, config.val_ratio)\n",
        "\n",
        "    print(f'学習セットのサンプル数: {len(train_set)}')\n",
        "    print(f'検証セットのサンプル数: {len(val_set)}')\n",
        "\n",
        "    # 学習時にランダムにサンプルするためのサンプラー\n",
        "    train_sampler = SubsetRandomSampler(train_set)\n",
        "\n",
        "    # DataLoaderを生成\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.batch_size,\n",
        "        num_workers=config.num_workers, sampler=train_sampler,\n",
        "        collate_fn=collate_func)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.batch_size,\n",
        "        num_workers=config.num_workers, sampler=val_set,\n",
        "        collate_fn=collate_func)\n",
        "\n",
        "    # RetinaNetの生成\n",
        "    model = RetinaNet(len(train_dataset.classes))\n",
        "    # ResNet18をImageNetの学習済みモデルで初期化\n",
        "    # 最後の全結合層がないなどのモデルの改変を許容するため、strict=False\n",
        "    model.backbone.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "        'https://download.pytorch.org/models/resnet18-5c106cde.pth'),\n",
        "                                   strict=False)\n",
        "\n",
        "    # モデルを指定デバイスに転送\n",
        "    model.to(config.device)\n",
        "\n",
        "    # Optimizerの生成\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n",
        "\n",
        "    # 指定したエポックで学習率を1/10に減衰するスケジューラを生成\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, milestones=[config.lr_drop], gamma=0.1)\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(train_loader) as pbar:\n",
        "            pbar.set_description(f'[エポック {epoch + 1}]')\n",
        "\n",
        "            # 移動平均計算用\n",
        "            losses_class = deque()\n",
        "            losses_box = deque()\n",
        "            losses = deque()\n",
        "            for imgs, targets in pbar:\n",
        "                imgs = imgs.to(model.get_device())\n",
        "                targets = [{k: v.to(model.get_device())\n",
        "                            for k, v in target.items()}\n",
        "                           for target in targets]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                preds_class, preds_box, anchors = model(imgs)\n",
        "\n",
        "                loss_class, loss_box = loss_func(\n",
        "                    preds_class, preds_box, anchors, targets)\n",
        "                loss = loss_class + loss_box\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # 勾配全体のL2ノルムが上限を超えるとき上限値でクリップ\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), config.clip)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                losses_class.append(loss_class.item())\n",
        "                losses_box.append(loss_box.item())\n",
        "                losses.append(loss.item())\n",
        "                if len(losses) > config.moving_avg:\n",
        "                    losses_class.popleft()\n",
        "                    losses_box.popleft()\n",
        "                    losses.popleft()\n",
        "                pbar.set_postfix({\n",
        "                    'loss': torch.Tensor(losses).mean().item(),\n",
        "                    'loss_class': torch.Tensor(\n",
        "                        losses_class).mean().item(),\n",
        "                    'loss_box': torch.Tensor(\n",
        "                        losses_box).mean().item()})\n",
        "\n",
        "        # スケジューラでエポック数をカウント\n",
        "        scheduler.step()\n",
        "\n",
        "        # パラメータを保存\n",
        "        torch.save(model.state_dict(), config.save_file)\n",
        "\n",
        "        # 検証\n",
        "        if (epoch + 1) % config.val_interval == 0:\n",
        "            evaluate(val_loader, model, loss_func)"
      ],
      "metadata": {
        "id": "Y3HGhxH4kKXY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "batch: CocoDetectionからサンプルした複数の画像とラベルをまとめたもの\n",
        "'''\n",
        "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, dict]]]):\n",
        "    # ミニバッチの中の画像で最大の高さと幅を取得\n",
        "    max_height = 0\n",
        "    max_width = 0\n",
        "    for img, _ in batch:\n",
        "        height, width = img.shape[1:]\n",
        "        max_height = max(max_height, height)\n",
        "        max_width = max(max_width, width)\n",
        "\n",
        "    # バックボーンネットワークで特徴マップの解像度を下げるときに\n",
        "    # 切り捨てが起きないように入力の幅と高さを32の倍数にしておく\n",
        "    # もし32の倍数でない場合、バックボーンネットワークの特徴マップと\n",
        "    # 特徴ピラミッドネットワークのアップスケーリングでできた特徴マップ\n",
        "    # の大きさに不整合が生じ、加算できなくなる\n",
        "    height = (max_height + 31) // 32 * 32\n",
        "    width = (max_width + 31) // 32 * 32\n",
        "\n",
        "    # 画像を一つにテンソルにまとめ、ラベルはリストに集約\n",
        "    imgs = batch[0][0].new_zeros((len(batch), 3, height, width))\n",
        "    targets = []\n",
        "    for i, (img, target) in enumerate(batch):\n",
        "        height, width = img.shape[1:]\n",
        "        imgs[i, :, :height, :width] = img\n",
        "\n",
        "        targets.append(target)\n",
        "\n",
        "    return imgs, targets"
      ],
      "metadata": {
        "id": "IqX5RYmrkOta"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "data_loader   : 評価に使うデータを読み込むデータローダ\n",
        "model         : 評価対象のモデル\n",
        "loss_func     : 目的関数\n",
        "conf_threshold: 信頼度の閾値\n",
        "nms_threshold : NMSのIoU閾値\n",
        "'''\n",
        "def evaluate(data_loader: DataLoader, model: nn.Module,\n",
        "             loss_func: Callable, conf_threshold: float=0.05,\n",
        "             nms_threshold: float=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    losses_class = []\n",
        "    losses_box = []\n",
        "    losses = []\n",
        "    preds = []\n",
        "    img_ids = []\n",
        "    for imgs, targets in tqdm(data_loader, desc='[Validation]'):\n",
        "        with  torch.no_grad():\n",
        "            imgs = imgs.to(model.get_device())\n",
        "            targets = [{k: v.to(model.get_device())\n",
        "                        for k, v in target.items()}\n",
        "                       for target in targets]\n",
        "\n",
        "            preds_class, preds_box, anchors = model(imgs)\n",
        "\n",
        "            loss_class, loss_box = loss_func(\n",
        "                preds_class, preds_box, anchors, targets)\n",
        "            loss = loss_class + loss_box\n",
        "\n",
        "            losses_class.append(loss_class)\n",
        "            losses_box.append(loss_box)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # 後処理により最終的な検出矩形を取得\n",
        "            scores, labels, boxes = post_process(\n",
        "                preds_class, preds_box, anchors, targets,\n",
        "                conf_threshold=conf_threshold,\n",
        "                nms_threshold=nms_threshold)\n",
        "\n",
        "            for img_scores, img_labels, img_boxes, img_targets in zip(\n",
        "                    scores, labels, boxes, targets):\n",
        "                img_ids.append(img_targets['image_id'].item())\n",
        "\n",
        "                # 評価のためにCOCOの元々の矩形表現である\n",
        "                # xmin, ymin, width, heightに変換\n",
        "                img_boxes[:, 2:] -= img_boxes[:, :2]\n",
        "\n",
        "                for score, label, box in zip(\n",
        "                        img_scores, img_labels, img_boxes):\n",
        "                    # COCO評価用のデータの保存\n",
        "                    preds.append({\n",
        "                        'image_id': img_targets['image_id'].item(),\n",
        "                        'category_id': \\\n",
        "                        data_loader.dataset.to_coco_label(\n",
        "                            label.item()),\n",
        "                        'score': score.item(),\n",
        "                        'bbox': box.to('cpu').numpy().tolist()\n",
        "                    })\n",
        "\n",
        "    loss_class = torch.stack(losses_class).mean().item()\n",
        "    loss_box = torch.stack(losses_box).mean().item()\n",
        "    loss = torch.stack(losses).mean().item()\n",
        "    print(f'Validation loss = {loss:.3f},'\n",
        "          f'class loss = {loss_class:.3f}, '\n",
        "          f'box loss = {loss_box:.3f} ')\n",
        "\n",
        "    if len(preds) == 0:\n",
        "        print('Nothing detected, skip evaluation.')\n",
        "\n",
        "        return\n",
        "\n",
        "    # COCOevalクラスを使って評価するには検出結果を\n",
        "    # jsonファイルに出力する必要があるため、jsonファイルに一時保存\n",
        "    with open('tmp.json', 'w') as f:\n",
        "        json.dump(preds, f)\n",
        "\n",
        "    # 一時保存した検出結果をCOCOクラスを使って読み込み\n",
        "    coco_results = data_loader.dataset.coco.loadRes('tmp.json')\n",
        "\n",
        "    # COCOevalクラスを使って評価\n",
        "    coco_eval = COCOeval(\n",
        "        data_loader.dataset.coco, coco_results, 'bbox')\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()"
      ],
      "metadata": {
        "id": "II9Ezf9_kT1D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/python_image_recognition/data/coco2014/gazou.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMdmHuWzkW38",
        "outputId": "0a5c7ba1-2bae-477e-b0de-b637716e7e83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/python_image_recognition/data/coco2014/gazou.zip\n",
            "   creating: gazou/\n",
            "  inflating: __MACOSX/._gazou        \n",
            "  inflating: gazou/IMG_134.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_134.jpeg  \n",
            "  inflating: gazou/IMG_94.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_94.jpeg  \n",
            "  inflating: gazou/IMG_57.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_57.jpeg  \n",
            "  inflating: gazou/IMG_118.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_118.jpeg  \n",
            "  inflating: gazou/IMG_16.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_16.jpeg  \n",
            "  inflating: gazou/IMG_61.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_61.jpeg  \n",
            "  inflating: gazou/IMG_180.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_180.jpeg  \n",
            "  inflating: gazou/IMG_20.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_20.jpeg  \n",
            "  inflating: gazou/IMG_77.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_77.jpeg  \n",
            "  inflating: gazou/IMG_102.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_102.jpeg  \n",
            "  inflating: gazou/IMG_6.jpeg        \n",
            "  inflating: __MACOSX/gazou/._IMG_6.jpeg  \n",
            "  inflating: gazou/IMG_154.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_154.jpeg  \n",
            "  inflating: gazou/IMG_178.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_178.jpeg  \n",
            "  inflating: gazou/IMG_206.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_206.jpeg  \n",
            "  inflating: gazou/IMG_139.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_139.jpeg  \n",
            "  inflating: gazou/IMG_60.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_60.jpeg  \n",
            "  inflating: gazou/IMG_17.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_17.jpeg  \n",
            "  inflating: gazou/IMG_40.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_40.jpeg  \n",
            "  inflating: gazou/IMG_56.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_56.jpeg  \n",
            "  inflating: gazou/IMG_83.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_83.jpeg  \n",
            "  inflating: gazou/IMG_128.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_128.jpeg  \n",
            "  inflating: gazou/IMG_153.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_153.jpeg  \n",
            "  inflating: gazou/IMG_1.jpeg        \n",
            "  inflating: __MACOSX/gazou/._IMG_1.jpeg  \n",
            "  inflating: gazou/IMG_112.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_112.jpeg  \n",
            "  inflating: gazou/IMG_84.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_84.jpeg  \n",
            "  inflating: gazou/IMG_132.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_132.jpeg  \n",
            "  inflating: gazou/IMG_124.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_124.jpeg  \n",
            "  inflating: gazou/IMG_92.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_92.jpeg  \n",
            "  inflating: gazou/IMG_51.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_51.jpeg  \n",
            "  inflating: gazou/IMG_47.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_47.jpeg  \n",
            "  inflating: gazou/IMG_10.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_10.jpeg  \n",
            "  inflating: gazou/IMG_11.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_11.jpeg  \n",
            "  inflating: gazou/IMG_93.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_93.jpeg  \n",
            "  inflating: gazou/IMG_164.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_164.jpeg  \n",
            "  inflating: gazou/IMG_85.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_85.jpeg  \n",
            "  inflating: gazou/IMG_144.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_144.jpeg  \n",
            "  inflating: gazou/IMG_105.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_105.jpeg  \n",
            "  inflating: gazou/IMG_70.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_70.jpeg  \n",
            "  inflating: gazou/IMG_200.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_200.jpeg  \n",
            "  inflating: gazou/IMG_31.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_31.jpeg  \n",
            "  inflating: gazou/IMG_168.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_168.jpeg  \n",
            "  inflating: gazou/IMG_66.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_66.jpeg  \n",
            "  inflating: gazou/IMG_187.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_187.jpeg  \n",
            "  inflating: gazou/IMG_89.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_89.jpeg  \n",
            "  inflating: gazou/IMG_49.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_49.jpeg  \n",
            "  inflating: gazou/IMG_147.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_147.jpeg  \n",
            "  inflating: gazou/IMG_106.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_106.jpeg  \n",
            "  inflating: gazou/IMG_203.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_203.jpeg  \n",
            "  inflating: gazou/IMG_184.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_184.jpeg  \n",
            "  inflating: gazou/IMG_65.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_65.jpeg  \n",
            "  inflating: gazou/IMG_12.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_12.jpeg  \n",
            "  inflating: gazou/IMG_53.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_53.jpeg  \n",
            "  inflating: gazou/IMG_126.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_126.jpeg  \n",
            "  inflating: gazou/IMG_167.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_167.jpeg  \n",
            "  inflating: gazou/IMG_189.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_189.jpeg  \n",
            "  inflating: gazou/IMG_68.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_68.jpeg  \n",
            "  inflating: gazou/IMG_127.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_127.jpeg  \n",
            "  inflating: gazou/IMG_91.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_91.jpeg  \n",
            "  inflating: gazou/IMG_170.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_170.jpeg  \n",
            "  inflating: gazou/IMG_52.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_52.jpeg  \n",
            "  inflating: gazou/IMG_13.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_13.jpeg  \n",
            "  inflating: gazou/IMG_185.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_185.jpeg  \n",
            "  inflating: gazou/IMG_64.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_64.jpeg  \n",
            "  inflating: gazou/IMG_193.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_193.jpeg  \n",
            "  inflating: gazou/IMG_146.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_146.jpeg  \n",
            "  inflating: gazou/IMG_111.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_111.jpeg  \n",
            "  inflating: gazou/IMG_9.jpeg        \n",
            "  inflating: __MACOSX/gazou/._IMG_9.jpeg  \n",
            "  inflating: gazou/IMG_55.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_55.jpeg  \n",
            "  inflating: gazou/IMG_198.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_198.jpeg  \n",
            "  inflating: gazou/IMG_177.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_177.jpeg  \n",
            "  inflating: gazou/IMG_120.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_120.jpeg  \n",
            "  inflating: gazou/IMG_38.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_38.jpeg  \n",
            "  inflating: gazou/IMG_136.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_136.jpeg  \n",
            "  inflating: gazou/IMG_161.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_161.jpeg  \n",
            "  inflating: gazou/IMG_5.jpeg        \n",
            "  inflating: __MACOSX/gazou/._IMG_5.jpeg  \n",
            "  inflating: gazou/IMG_59.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_59.jpeg  \n",
            "  inflating: gazou/IMG_75.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_75.jpeg  \n",
            "  inflating: gazou/IMG_22.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_22.jpeg  \n",
            "  inflating: gazou/IMG_35.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_35.jpeg  \n",
            "  inflating: gazou/IMG_212.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_212.jpeg  \n",
            "  inflating: gazou/IMG_74.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_74.jpeg  \n",
            "  inflating: gazou/IMG_58.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_58.jpeg  \n",
            "  inflating: gazou/IMG_81.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_81.jpeg  \n",
            "  inflating: gazou/IMG_160.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_160.jpeg  \n",
            "  inflating: gazou/IMG_176.jpeg      \n",
            "  inflating: __MACOSX/gazou/._IMG_176.jpeg  \n",
            "  inflating: gazou/IMG_54.jpeg       \n",
            "  inflating: __MACOSX/gazou/._IMG_54.jpeg  \n",
            "  inflating: gazou/IMG_8.jpeg        \n",
            "  inflating: __MACOSX/gazou/._IMG_8.jpeg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG9itARQkiU8",
        "outputId": "85b91016-64f8-42b0-93cd-5ba02df77823"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "学習セットのサンプル数: 70\n",
            "検証セットのサンプル数: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 279MB/s]\n",
            "[エポック 1]: 100%|██████████| 9/9 [00:23<00:00,  2.67s/it, loss=1.32, loss_class=1.21, loss_box=0.109]\n",
            "[エポック 2]: 100%|██████████| 9/9 [00:19<00:00,  2.16s/it, loss=1.26, loss_class=1.16, loss_box=0.106]\n",
            "[エポック 3]: 100%|██████████| 9/9 [00:20<00:00,  2.28s/it, loss=1.2, loss_class=1.1, loss_box=0.0939]\n",
            "[エポック 4]: 100%|██████████| 9/9 [00:19<00:00,  2.12s/it, loss=1.13, loss_class=1.04, loss_box=0.0898]\n",
            "[エポック 5]: 100%|██████████| 9/9 [00:20<00:00,  2.28s/it, loss=0.949, loss_class=0.862, loss_box=0.0865]\n",
            "[Validation]: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 1.307,class loss = 1.203, box loss = 0.104 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.03s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.100\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.029\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.064\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 6]: 100%|██████████| 9/9 [00:19<00:00,  2.21s/it, loss=0.853, loss_class=0.77, loss_box=0.0832]\n",
            "[エポック 7]: 100%|██████████| 9/9 [00:20<00:00,  2.25s/it, loss=0.763, loss_class=0.685, loss_box=0.0783]\n",
            "[エポック 8]: 100%|██████████| 9/9 [00:28<00:00,  3.15s/it, loss=0.696, loss_class=0.623, loss_box=0.0725]\n",
            "[エポック 9]: 100%|██████████| 9/9 [00:21<00:00,  2.41s/it, loss=0.639, loss_class=0.57, loss_box=0.0694]\n",
            "[エポック 10]: 100%|██████████| 9/9 [00:22<00:00,  2.49s/it, loss=0.616, loss_class=0.549, loss_box=0.0671]\n",
            "[Validation]: 100%|██████████| 3/3 [00:05<00:00,  1.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 1.016,class loss = 0.940, box loss = 0.076 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.10s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.044\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.136\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.107\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.114\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 11]: 100%|██████████| 9/9 [00:21<00:00,  2.36s/it, loss=0.575, loss_class=0.512, loss_box=0.063]\n",
            "[エポック 12]: 100%|██████████| 9/9 [00:22<00:00,  2.52s/it, loss=0.559, loss_class=0.5, loss_box=0.0597]\n",
            "[エポック 13]: 100%|██████████| 9/9 [00:22<00:00,  2.50s/it, loss=0.529, loss_class=0.471, loss_box=0.0578]\n",
            "[エポック 14]: 100%|██████████| 9/9 [00:23<00:00,  2.60s/it, loss=0.487, loss_class=0.435, loss_box=0.0527]\n",
            "[エポック 15]: 100%|██████████| 9/9 [00:24<00:00,  2.68s/it, loss=0.475, loss_class=0.427, loss_box=0.048]\n",
            "[Validation]: 100%|██████████| 3/3 [00:05<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 0.994,class loss = 0.924, box loss = 0.071 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.06s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.117\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.253\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.111\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.117\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.154\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.196\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.196\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 16]: 100%|██████████| 9/9 [00:23<00:00,  2.60s/it, loss=0.457, loss_class=0.407, loss_box=0.0503]\n",
            "[エポック 17]: 100%|██████████| 9/9 [00:19<00:00,  2.16s/it, loss=0.427, loss_class=0.38, loss_box=0.0467]\n",
            "[エポック 18]: 100%|██████████| 9/9 [00:19<00:00,  2.12s/it, loss=0.4, loss_class=0.355, loss_box=0.045]\n",
            "[エポック 19]: 100%|██████████| 9/9 [00:23<00:00,  2.62s/it, loss=0.382, loss_class=0.341, loss_box=0.0413]\n",
            "[エポック 20]: 100%|██████████| 9/9 [00:22<00:00,  2.49s/it, loss=0.375, loss_class=0.331, loss_box=0.0444]\n",
            "[Validation]: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 0.985,class loss = 0.917, box loss = 0.067 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.06s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.119\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.033\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.119\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.143\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 21]: 100%|██████████| 9/9 [00:22<00:00,  2.45s/it, loss=0.336, loss_class=0.295, loss_box=0.0402]\n",
            "[エポック 22]: 100%|██████████| 9/9 [00:20<00:00,  2.29s/it, loss=0.33, loss_class=0.289, loss_box=0.0409]\n",
            "[エポック 23]: 100%|██████████| 9/9 [00:21<00:00,  2.42s/it, loss=0.324, loss_class=0.286, loss_box=0.0377]\n",
            "[エポック 24]: 100%|██████████| 9/9 [00:23<00:00,  2.60s/it, loss=0.302, loss_class=0.267, loss_box=0.0349]\n",
            "[エポック 25]: 100%|██████████| 9/9 [00:22<00:00,  2.56s/it, loss=0.29, loss_class=0.253, loss_box=0.0367]\n",
            "[Validation]: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 0.997,class loss = 0.933, box loss = 0.063 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.07s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.469\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.080\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.206\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.201\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.240\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 26]: 100%|██████████| 9/9 [00:23<00:00,  2.59s/it, loss=0.262, loss_class=0.231, loss_box=0.0313]\n",
            "[エポック 27]: 100%|██████████| 9/9 [00:20<00:00,  2.30s/it, loss=0.275, loss_class=0.242, loss_box=0.0335]\n",
            "[エポック 28]: 100%|██████████| 9/9 [00:23<00:00,  2.61s/it, loss=0.248, loss_class=0.215, loss_box=0.0328]\n",
            "[エポック 29]: 100%|██████████| 9/9 [00:23<00:00,  2.66s/it, loss=0.246, loss_class=0.211, loss_box=0.0355]\n",
            "[エポック 30]: 100%|██████████| 9/9 [00:21<00:00,  2.36s/it, loss=0.227, loss_class=0.194, loss_box=0.0331]\n",
            "[Validation]: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 1.081,class loss = 1.012, box loss = 0.068 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.444\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.137\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.207\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.232\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 31]: 100%|██████████| 9/9 [00:22<00:00,  2.51s/it, loss=0.214, loss_class=0.182, loss_box=0.0322]\n",
            "[エポック 32]: 100%|██████████| 9/9 [00:21<00:00,  2.37s/it, loss=0.198, loss_class=0.167, loss_box=0.031]\n",
            "[エポック 33]: 100%|██████████| 9/9 [00:19<00:00,  2.22s/it, loss=0.22, loss_class=0.189, loss_box=0.0317]\n",
            "[エポック 34]: 100%|██████████| 9/9 [00:22<00:00,  2.45s/it, loss=0.186, loss_class=0.158, loss_box=0.0286]\n",
            "[エポック 35]: 100%|██████████| 9/9 [00:20<00:00,  2.23s/it, loss=0.187, loss_class=0.159, loss_box=0.0273]\n",
            "[Validation]: 100%|██████████| 3/3 [00:06<00:00,  2.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 1.121,class loss = 1.053, box loss = 0.067 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.06s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.470\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.095\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.180\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.211\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 36]: 100%|██████████| 9/9 [00:19<00:00,  2.16s/it, loss=0.178, loss_class=0.15, loss_box=0.0281]\n",
            "[エポック 37]: 100%|██████████| 9/9 [00:21<00:00,  2.42s/it, loss=0.18, loss_class=0.148, loss_box=0.0316]\n",
            "[エポック 38]: 100%|██████████| 9/9 [00:19<00:00,  2.19s/it, loss=0.171, loss_class=0.141, loss_box=0.0305]\n",
            "[エポック 39]: 100%|██████████| 9/9 [00:21<00:00,  2.37s/it, loss=0.172, loss_class=0.145, loss_box=0.0273]\n",
            "[エポック 40]: 100%|██████████| 9/9 [00:19<00:00,  2.14s/it, loss=0.158, loss_class=0.13, loss_box=0.028]\n",
            "[Validation]: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 1.083,class loss = 1.025, box loss = 0.058 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.06s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.425\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.134\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.225\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.286\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 41]: 100%|██████████| 9/9 [00:19<00:00,  2.14s/it, loss=0.173, loss_class=0.145, loss_box=0.0283]\n",
            "[エポック 42]: 100%|██████████| 9/9 [00:19<00:00,  2.15s/it, loss=0.147, loss_class=0.121, loss_box=0.0257]\n",
            "[エポック 43]: 100%|██████████| 9/9 [00:21<00:00,  2.41s/it, loss=0.162, loss_class=0.134, loss_box=0.0276]\n",
            "[エポック 44]: 100%|██████████| 9/9 [00:19<00:00,  2.20s/it, loss=0.13, loss_class=0.106, loss_box=0.0245]\n",
            "[エポック 45]: 100%|██████████| 9/9 [00:22<00:00,  2.52s/it, loss=0.152, loss_class=0.129, loss_box=0.0238]\n",
            "[Validation]: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 1.120,class loss = 1.062, box loss = 0.058 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.454\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.167\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.184\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.190\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[エポック 46]: 100%|██████████| 9/9 [00:23<00:00,  2.64s/it, loss=0.149, loss_class=0.124, loss_box=0.0255]\n",
            "[エポック 47]: 100%|██████████| 9/9 [00:20<00:00,  2.24s/it, loss=0.113, loss_class=0.0923, loss_box=0.0205]\n",
            "[エポック 48]: 100%|██████████| 9/9 [00:21<00:00,  2.40s/it, loss=0.111, loss_class=0.0935, loss_box=0.018]\n",
            "[エポック 49]: 100%|██████████| 9/9 [00:19<00:00,  2.18s/it, loss=0.0913, loss_class=0.0738, loss_box=0.0175]\n",
            "[エポック 50]: 100%|██████████| 9/9 [00:22<00:00,  2.46s/it, loss=0.0828, loss_class=0.0659, loss_box=0.0169]\n",
            "[Validation]: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss = 1.065,class loss = 1.009, box loss = 0.056 \n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.494\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.246\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.257\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.257\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigDemo:\n",
        "    '''\n",
        "    ハイパーパラメータとオプションの設定\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.img_directory = 'drive/MyDrive/python_image_recognition/data/' \\\n",
        "                            'test_gazou' # 画像があるディレクトリ\n",
        "        self.load_file = 'drive/MyDrive/python_image_recognition/5_object_detection/model/' \\\n",
        "                            'retinanet.pth'    # 学習済みパラメータのパス\n",
        "        self.classes = ['kiyomizu_temple','八坂神社', '京都タワー','銀閣寺','円山公園','京都国立近代美術館','三十三間堂','養源院','法住寺','関西日仏学館','東本願寺','勝牛','宗忠神社','哲学の道','陽成天皇 神楽岡東陵','銀閣寺橋','建仁寺','京都大学','知恩院','法然院','東大路通','吉田神社','冷泉天皇 櫻本陵','池','平安神宮','祇園通り_花見小路通','後一條天皇 菩提樹院陵','京都市京セラ美術館','長楽寺']       #     # 検出対象の物体クラス\n",
        "        self.device = 'cuda'                   # デモに使うデバイス\n",
        "        self.conf_threshold = 0.6              # 検出における信頼度の閾値\n",
        "        self.nms_threshold = 0.5               # 検出におけるNMSのIoU閾値"
      ],
      "metadata": {
        "id": "7wv5V7qDrKmI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        ""
      ],
      "metadata": {
        "id": "bKgP5NhZTA_6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo():\n",
        "    config = ConfigDemo()\n",
        "\n",
        "    transforms = T.Compose((\n",
        "        T.RandomResize((608,), max_size=1024),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)),\n",
        "    ))\n",
        "\n",
        "    # 学習済みのモデルパラメータを読み込み\n",
        "    model = RetinaNet(len(config.classes))\n",
        "    model.load_state_dict(torch.load(config.load_file))\n",
        "    model.to(config.device)\n",
        "    model.eval()\n",
        "\n",
        "    for img_path in Path(config.img_directory).iterdir():\n",
        "        img_orig = Image.open(img_path)\n",
        "        width, height = img_orig.size\n",
        "\n",
        "        # データ整形を適用するためにダミーのラベルを作成\n",
        "        target = {\n",
        "            'classes': torch.zeros((0,), dtype=torch.int64),\n",
        "            'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
        "            'size': torch.tensor((width, height), dtype=torch.int64),\n",
        "            'orig_size': torch.tensor(\n",
        "                (width, height), dtype=torch.int64),\n",
        "        }\n",
        "\n",
        "        # データ整形\n",
        "        img, target = transforms(img_orig, target)\n",
        "        imgs, targets = collate_func([(img, target)])\n",
        "\n",
        "        with  torch.no_grad():\n",
        "            imgs = imgs.to(model.get_device())\n",
        "            targets = [{k: v.to(model.get_device())\n",
        "                        for k, v in target.items()}\n",
        "                       for target in targets]\n",
        "\n",
        "            preds_class, preds_box, anchors = model(imgs)\n",
        "\n",
        "            scores, labels, boxes = post_process(\n",
        "                preds_class, preds_box, anchors, targets,\n",
        "                conf_threshold=config.conf_threshold,\n",
        "                nms_threshold=config.nms_threshold)\n",
        "\n",
        "            # 描画用の画像を用意\n",
        "            img = torch.tensor(np.asarray(img_orig))\n",
        "            img = img.permute(2, 0, 1)\n",
        "\n",
        "            # クラスIDをクラス名に変換\n",
        "            labels = [config.classes[label] for label in labels[0]]\n",
        "\n",
        "            # 矩形を描画\n",
        "            img = draw_bounding_boxes(\n",
        "                img, boxes[0], labels, colors='red',\n",
        "                font='LiberationSans-Regular.ttf',\n",
        "                font_size=42, width=4)\n",
        "            img = img.permute(1, 2, 0)\n",
        "            img = img.to('cpu').numpy()\n",
        "            img = Image.fromarray(img)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "nI-z6nbwsEB_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo()"
      ],
      "metadata": {
        "id": "ECi6LLdZsF28"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsaniZjTi9PF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
